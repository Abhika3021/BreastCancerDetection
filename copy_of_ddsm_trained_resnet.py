# -*- coding: utf-8 -*-
"""Copy of ddsm_trained_resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jlf7WbseKfmjS5xiVOTQapTVgNKqMH_w
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.metrics import categorical_crossentropy

from keras.models import Sequential, Model
from keras.layers import Conv2D,MaxPooling2D,GlobalAveragePooling2D
from keras.layers import Dense,Flatten,Dropout,Activation,BatchNormalization,AvgPool2D,MaxPool2D
from tensorflow.keras.optimizers import Adam
import cv2

from tensorflow.keras.applications.resnet50 import preprocess_input,decode_predictions,ResNet50
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import os
imagelist=[]
filepath_images = "/content/drive/MyDrive/ddsm dataset split/train/Cancer"
imagelist=os.listdir(filepath_images)
fileimgarr=[]

for i in imagelist:
        fileimgprint=os.path.join("/content/drive/MyDrive/ddsm dataset split/train/Cancer",str(i))
        fileimgarr.append(fileimgprint)

print ('filepaths: ', len(fileimgarr))

df = pd.DataFrame({"file_paths":fileimgarr})
df.head()

plt.figure(figsize=(12,12))
for i in range(15):
    random = np.random.randint(1,len(fileimgarr))
    plt.subplot(3,5,i+1)
    plt.imshow(cv2.imread(df.loc[random,"file_paths"]))
    plt.xticks([])
    plt.yticks([])

plt.show()

train_data='/content/drive/MyDrive/ddsm dataset split/train'
test_data='/content/drive/MyDrive/ddsm dataset split/test'
valid_data='/content/drive/MyDrive/ddsm dataset split/val'

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,
                                   shear_range = 0.2, zoom_range = 0.2,
                                   horizontal_flip = True,validation_split=0.4)

train_generator = train_datagen.flow_from_directory(train_data,
                                              target_size = (224,224), batch_size =512,
                                              class_mode = 'categorical',subset='training')

val_generator = train_datagen.flow_from_directory(valid_data,
                                              target_size = (224,224), batch_size = 512,
                                              class_mode = 'categorical',subset='validation')

test_generator = train_datagen.flow_from_directory(test_data,
                                              target_size = (224,224), batch_size = 1,
                                              class_mode = 'categorical',subset='validation')

x,y=test_generator.next()
x.shape

base_model = ResNet50(include_top=False,weights='imagenet')
x=base_model.output
x=GlobalAveragePooling2D()(x)
x=Dense(1024,activation='relu')(x)
predictions=Dense(train_generator.num_classes,activation='softmax')(x)
model=Model(inputs=base_model.input,outputs=predictions)

model.summary()

for layer in base_model.layers:
  layer.trainable=False

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
hist = model.fit(train_generator,epochs=55)

model.save('/content/drive/MyDrive/projet dataset split/Resnet50 trained 55.h5')

test_loss,test_acc = model.evaluate(test_generator,verbose=2)
print('test accuracy : ',test_acc)

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['loss'])
plt.title('model_accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.show()

import seaborn as sns

model= tf.keras.models.load_model('/content/drive/MyDrive/projet dataset split/Resnet50 trained.h5')

filenames=test_generator.filenames
nb_samples=len(test_generator)
y_prob=[]
y_act=[]
test_generator.reset()
for i in range(nb_samples):
  X_test,Y_test=test_generator.next()
  y_prob.append(model.predict(X_test))
  y_act.append(Y_test)

predicted_class=[list(train_generator.class_indices.keys())[i.argmax()] for i in y_prob]
actual_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_act]

out_df = pd.DataFrame(np.vstack([predicted_class,actual_class]).T,columns=['predicted_class','actual_class'])
confusion_matrix=pd.crosstab(out_df['actual_class'],out_df['predicted_class'],rownames=['actual'],colnames=['predicted'])

sns.heatmap(confusion_matrix,cmap='Blues',annot=True,fmt='d')
plt.show()
print('test_accuracy : {} '.format((np.diagonal(confusion_matrix).sum()/confusion_matrix.sum().sum()*100)))